{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed74c789-341c-414b-a2a9-49fc566286d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in ./my-jupyter-env/lib/python3.13/site-packages (2025.7.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in ./my-jupyter-env/lib/python3.13/site-packages (from s3fs) (2.23.2)\n",
      "Requirement already satisfied: fsspec==2025.7.0 in ./my-jupyter-env/lib/python3.13/site-packages (from s3fs) (2025.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./my-jupyter-env/lib/python3.13/site-packages (from s3fs) (3.12.14)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.39.9,>=1.39.7 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.39.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./my-jupyter-env/lib/python3.13/site-packages (from botocore<1.39.9,>=1.39.7->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./my-jupyter-env/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./my-jupyter-env/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e907184-8631-4b38-b94a-b7765af7b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from kafka import KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "import os\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85462b0f-fc49-4435-bfeb-d6676e84b6c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlogging\u001b[49m.basicConfig(\n\u001b[32m      3\u001b[39m     level=logging.INFO,\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEarthquakeConsumer\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EarthquakeConsumer:\n",
    "    def _init_(self, bootstrap_servers=['13.204.84.130:9092'], topic_name='earthquake-events', \n",
    "                 s3_bucket='your-earthquake-bucket', consumer_group='earthquake-processors'):\n",
    "        \n",
    "        self.topic_name = topic_name\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.consumer_group = consumer_group\n",
    "        \n",
    "        # Initialize Kafka Consumer\n",
    "        self.consumer = KafkaConsumer(\n",
    "            self.topic_name,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            group_id=self.consumer_group,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            key_deserializer=lambda m: m.decode('utf-8') if m else None,\n",
    "            auto_offset_reset='latest',\n",
    "            enable_auto_commit=True,\n",
    "            auto_commit_interval_ms=5000,\n",
    "            max_poll_records=100,\n",
    "            consumer_timeout_ms=10000\n",
    "        )\n",
    "        \n",
    "        # Initialize S3 client (uses AWS CLI credentials)\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Buffer for batch uploads\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 50  # Upload every 50 messages\n",
    "        self.last_upload_time = time.time()\n",
    "        self.upload_interval = 300  # Upload every 5 minutes regardless\n",
    "        \n",
    "    def validate_earthquake_data(self, data):\n",
    "        \"\"\"Validate earthquake data structure\"\"\"\n",
    "        required_fields = ['event_id', 'event_time', 'magnitude', 'coordinates']\n",
    "        \n",
    "        for field in required_fields:\n",
    "            if field not in data:\n",
    "                logger.warning(f\"Missing required field: {field}\")\n",
    "                return False\n",
    "                \n",
    "        # Validate coordinate structure\n",
    "        if not isinstance(data['coordinates'], dict):\n",
    "            logger.warning(\"Invalid coordinates structure\")\n",
    "            return False\n",
    "            \n",
    "        required_coords = ['latitude', 'longitude', 'depth_km']\n",
    "        for coord in required_coords:\n",
    "            if coord not in data['coordinates']:\n",
    "                logger.warning(f\"Missing coordinate field: {coord}\")\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    def enrich_for_storage(self, earthquake_data):\n",
    "        \"\"\"Add storage-specific metadata\"\"\"\n",
    "        enriched = earthquake_data.copy()\n",
    "        \n",
    "        # Add partition information for S3 organization\n",
    "        event_time = datetime.fromisoformat(earthquake_data['event_time'].replace('Z', '+00:00'))\n",
    "        \n",
    "        enriched.update({\n",
    "            'storage_metadata': {\n",
    "                'ingestion_time': datetime.utcnow().isoformat(),\n",
    "                'partition_year': event_time.year,\n",
    "                'partition_month': event_time.month,\n",
    "                'partition_day': event_time.day,\n",
    "                'partition_hour': event_time.hour\n",
    "            },\n",
    "            'data_quality': {\n",
    "                'has_magnitude': earthquake_data.get('magnitude') is not None,\n",
    "                'has_location': all(earthquake_data['coordinates'].get(k) is not None \n",
    "                                 for k in ['latitude', 'longitude']),\n",
    "                'has_depth': earthquake_data['coordinates'].get('depth_km') is not None,\n",
    "                'significance_score': earthquake_data.get('significance', 0)\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return enriched\n",
    "    \n",
    "    def generate_s3_key(self, earthquake_data):\n",
    "        \"\"\"Generate S3 key with partitioning structure\"\"\"\n",
    "        metadata = earthquake_data['storage_metadata']\n",
    "        \n",
    "        # Partitioned by date for efficient querying\n",
    "        s3_key = (f\"earthquake-data/\"\n",
    "                 f\"year={metadata['partition_year']}/\"\n",
    "                 f\"month={metadata['partition_month']:02d}/\"\n",
    "                 f\"day={metadata['partition_day']:02d}/\"\n",
    "                 f\"hour={metadata['partition_hour']:02d}/\"\n",
    "                 f\"{earthquake_data['event_id']}.json\")\n",
    "        \n",
    "        return s3_key\n",
    "    \n",
    "    def upload_to_s3(self, earthquake_data):\n",
    "        \"\"\"Upload single earthquake record to S3\"\"\"\n",
    "        try:\n",
    "            s3_key = self.generate_s3_key(earthquake_data)\n",
    "            \n",
    "            # Convert to JSON and compress\n",
    "            json_data = json.dumps(earthquake_data, indent=2)\n",
    "            \n",
    "            # Upload to S3\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Key=s3_key,\n",
    "                Body=json_data,\n",
    "                ContentType='application/json',\n",
    "                Metadata={\n",
    "                    'event-id': earthquake_data['event_id'],\n",
    "                    'magnitude': str(earthquake_data.get('magnitude', 'unknown')),\n",
    "                    'source': 'kafka-consumer',\n",
    "                    'ingestion-time': earthquake_data['storage_metadata']['ingestion_time']\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Uploaded earthquake {earthquake_data['event_id']} to S3: {s3_key}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to upload to S3: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def batch_upload_to_s3(self, earthquake_batch):\n",
    "        \"\"\"Upload batch of earthquakes as compressed JSONL file\"\"\"\n",
    "        try:\n",
    "            if not earthquake_batch:\n",
    "                return True\n",
    "                \n",
    "            # Create batch metadata\n",
    "            batch_time = datetime.utcnow()\n",
    "            first_event_time = earthquake_batch[0]['event_time']\n",
    "            \n",
    "            # Generate batch S3 key\n",
    "            s3_key = (f\"earthquake-data/batches/\"\n",
    "                     f\"year={batch_time.year}/\"\n",
    "                     f\"month={batch_time.month:02d}/\"\n",
    "                     f\"day={batch_time.day:02d}/\"\n",
    "                     f\"batch_{batch_time.strftime('%Y%m%d_%H%M%S')}_{len(earthquake_batch)}_records.jsonl.gz\")\n",
    "            \n",
    "            # Create compressed JSONL\n",
    "            buffer = BytesIO()\n",
    "            with gzip.GzipFile(fileobj=buffer, mode='wb') as gz_file:\n",
    "                for earthquake in earthquake_batch:\n",
    "                    line = json.dumps(earthquake) + '\\n'\n",
    "                    gz_file.write(line.encode('utf-8'))\n",
    "            \n",
    "            # Upload to S3\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Key=s3_key,\n",
    "                Body=buffer.getvalue(),\n",
    "                ContentType='application/gzip',\n",
    "                Metadata={\n",
    "                    'batch-size': str(len(earthquake_batch)),\n",
    "                    'first-event-time': first_event_time,\n",
    "                    'batch-upload-time': batch_time.isoformat(),\n",
    "                    'compression': 'gzip'\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Uploaded batch of {len(earthquake_batch)} earthquakes to S3: {s3_key}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to upload batch to S3: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def should_upload_buffer(self):\n",
    "        \"\"\"Check if buffer should be uploaded\"\"\"\n",
    "        buffer_full = len(self.buffer) >= self.buffer_size\n",
    "        time_elapsed = time.time() - self.last_upload_time > self.upload_interval\n",
    "        return buffer_full or time_elapsed\n",
    "    \n",
    "    def flush_buffer(self):\n",
    "        \"\"\"Upload buffered records to S3\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Flushing buffer with {len(self.buffer)} records\")\n",
    "        \n",
    "        if self.batch_upload_to_s3(self.buffer):\n",
    "            self.buffer.clear()\n",
    "            self.last_upload_time = time.time()\n",
    "        else:\n",
    "            # Fallback: try individual uploads\n",
    "            logger.info(\"Batch upload failed, trying individual uploads...\")\n",
    "            successful_uploads = 0\n",
    "            failed_records = []\n",
    "            \n",
    "            for earthquake in self.buffer:\n",
    "                if self.upload_to_s3(earthquake):\n",
    "                    successful_uploads += 1\n",
    "                else:\n",
    "                    failed_records.append(earthquake)\n",
    "            \n",
    "            logger.info(f\"Individual uploads: {successful_uploads} successful, {len(failed_records)} failed\")\n",
    "            \n",
    "            # Keep failed records for retry\n",
    "            self.buffer = failed_records\n",
    "            if successful_uploads > 0:\n",
    "                self.last_upload_time = time.time()\n",
    "    \n",
    "    def process_message(self, message):\n",
    "        \"\"\"Process individual Kafka message\"\"\"\n",
    "        try:\n",
    "            earthquake_data = message.value\n",
    "            \n",
    "            # Validate data\n",
    "            if not self.validate_earthquake_data(earthquake_data):\n",
    "                logger.warning(f\"Invalid earthquake data received: {message.key}\")\n",
    "                return False\n",
    "            \n",
    "            # Enrich data for storage\n",
    "            enriched_data = self.enrich_for_storage(earthquake_data)\n",
    "            \n",
    "            # Add to buffer\n",
    "            self.buffer.append(enriched_data)\n",
    "            \n",
    "            # Log high-significance earthquakes immediately\n",
    "            if enriched_data.get('magnitude', 0) >= 5.0:\n",
    "                logger.warning(f\"SIGNIFICANT EARTHQUAKE: Magnitude {enriched_data['magnitude']} \"\n",
    "                             f\"at {enriched_data['place']} - {enriched_data['event_id']}\")\n",
    "            \n",
    "            logger.info(f\"Processed earthquake: {enriched_data['event_id']} \"\n",
    "                       f\"(Mag: {enriched_data.get('magnitude', 'N/A')}) - Buffer: {len(self.buffer)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing message: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_consumer(self):\n",
    "        \"\"\"Main consumer loop\"\"\"\n",
    "        logger.info(f\"Starting earthquake consumer for topic: {self.topic_name}\")\n",
    "        logger.info(f\"Storing data to S3 bucket: {self.s3_bucket}\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                # Poll for messages\n",
    "                message_batch = self.consumer.poll(timeout_ms=5000)\n",
    "                \n",
    "                if message_batch:\n",
    "                    for topic_partition, messages in message_batch.items():\n",
    "                        for message in messages:\n",
    "                            self.process_message(message)\n",
    "                \n",
    "                # Check if buffer should be uploaded\n",
    "                if self.should_upload_buffer():\n",
    "                    self.flush_buffer()\n",
    "                \n",
    "                # Periodic status log\n",
    "                if len(self.buffer) > 0:\n",
    "                    logger.debug(f\"Buffer status: {len(self.buffer)} records pending upload\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Received shutdown signal...\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error in consumer loop: {e}\")\n",
    "        finally:\n",
    "            # Flush any remaining buffer\n",
    "            if self.buffer:\n",
    "                logger.info(\"Flushing final buffer before shutdown...\")\n",
    "                self.flush_buffer()\n",
    "            \n",
    "            self.consumer.close()\n",
    "            logger.info(\"Consumer closed\")\n",
    "\n",
    "def main():\n",
    "    # Configuration - Update these values\n",
    "    KAFKA_BOOTSTRAP_SERVERS = ['13.204.84.130:9092']\n",
    "    TOPIC_NAME = 'earthquake-events'\n",
    "    S3_BUCKET = 'your-earthquake-data-bucket'  # Update this!\n",
    "    CONSUMER_GROUP = 'earthquake-s3-processors'\n",
    "    \n",
    "    # Verify S3 bucket exists\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=S3_BUCKET)\n",
    "        logger.info(f\"S3 bucket '{S3_BUCKET}' is accessible\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cannot access S3 bucket '{S3_BUCKET}': {e}\")\n",
    "        logger.error(\"Please create the bucket or update the bucket name in the script\")\n",
    "        return\n",
    "    \n",
    "    # Create and run consumer\n",
    "    consumer = EarthquakeConsumer(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        topic_name=TOPIC_NAME,\n",
    "        s3_bucket=S3_BUCKET,\n",
    "        consumer_group=CONSUMER_GROUP\n",
    "    )\n",
    "    \n",
    "    consumer.run_consumer()\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609eb737-dd82-4717-a866-6e26c24d5fb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     63\u001b[39m         consumer.close()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_name_\u001b[49m == \u001b[33m\"\u001b[39m\u001b[33m_main_\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     66\u001b[39m     main()\n",
      "\u001b[31mNameError\u001b[39m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from kafka import KafkaConsumer\n",
    "from datetime import datetime\n",
    "\n",
    "# ======= CONFIGURATION - UPDATE THESE =======\n",
    "KAFKA_SERVERS = ['YOUR_EC2_IP:9092']  # Change this!\n",
    "TOPIC_NAME = 'earthquake-events'      # Change this!  \n",
    "S3_BUCKET = 'your-earthquake-bucket'  # Change this!\n",
    "\n",
    "def upload_to_s3(s3_client, earthquake_data):\n",
    "    \"\"\"Upload earthquake to S3\"\"\"\n",
    "    try:\n",
    "        # Create S3 key (file path)\n",
    "        event_time = datetime.fromtimestamp(earthquake_data['time'] / 1000)\n",
    "        s3_key = f\"earthquakes/{event_time.year}/{event_time.month:02d}/{event_time.day:02d}/{earthquake_data['id']}.json\"\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_client.put_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=s3_key,\n",
    "            Body=json.dumps(earthquake_data, indent=2),\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        \n",
    "        print(f\"Uploaded: {earthquake_data['id']} to S3\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"S3 upload failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Create Kafka consumer\n",
    "    consumer = KafkaConsumer(\n",
    "        TOPIC_NAME,\n",
    "        bootstrap_servers=KAFKA_SERVERS,\n",
    "        group_id='earthquake-consumer',\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='latest'\n",
    "    )\n",
    "    \n",
    "    # Create S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    print(f\"Starting earthquake consumer...\")\n",
    "    print(f\"Kafka: {KAFKA_SERVERS}\")\n",
    "    print(f\"Topic: {TOPIC_NAME}\")\n",
    "    print(f\"S3 Bucket: {S3_BUCKET}\")\n",
    "    \n",
    "    try:\n",
    "        for message in consumer:\n",
    "            earthquake_data = message.value\n",
    "            \n",
    "            print(f\"Received: {earthquake_data['id']} - Mag {earthquake_data['magnitude']}\")\n",
    "            \n",
    "            # Upload to S3\n",
    "            upload_to_s3(s3_client, earthquake_data)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping consumer...\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbcbeeb4-792f-4163-a952-21de5ba96994",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoBrokersAvailable\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m S3_BUCKET = \u001b[33m'\u001b[39m\u001b[33myour-earthquake-bucket\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create consumer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m consumer = \u001b[43mKafkaConsumer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOPIC_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mKAFKA_SERVERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_deserializer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Create S3 connection\u001b[39;00m\n\u001b[32m     18\u001b[39m s3 = S3FileSystem()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/consumer/group.py:387\u001b[39m, in \u001b[36mKafkaConsumer.__init__\u001b[39m\u001b[34m(self, *topics, **configs)\u001b[39m\n\u001b[32m    383\u001b[39m         \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, str_version.split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m    384\u001b[39m     log.warning(\u001b[33m'\u001b[39m\u001b[33muse api_version=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m [tuple] -- \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m as str is deprecated\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    385\u001b[39m                 \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m]), str_version)\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m \u001b[38;5;28mself\u001b[39m._client = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkafka_client\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Get auto-discovered / normalized version from client\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m._client.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/client_async.py:262\u001b[39m, in \u001b[36mKafkaClient.__init__\u001b[39m\u001b[34m(self, **configs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Check Broker Version if not set explicitly\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m BROKER_API_VERSIONS:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mself\u001b[39m._api_versions = BROKER_API_VERSIONS[\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/client_async.py:1074\u001b[39m, in \u001b[36mKafkaClient.check_version\u001b[39m\u001b[34m(self, node_id, timeout, **kwargs)\u001b[39m\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Errors.NodeNotReadyError(node_id)\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1074\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Errors.NoBrokersAvailable()\n",
      "\u001b[31mNoBrokersAvailable\u001b[39m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "# Configuration\n",
    "KAFKA_SERVERS = ['YOUR_EC2_IP:9092']\n",
    "TOPIC_NAME = 'earthquake-events'\n",
    "S3_BUCKET = 'your-earthquake-bucket'\n",
    "\n",
    "# Create consumer\n",
    "consumer = KafkaConsumer(\n",
    "    TOPIC_NAME,\n",
    "    bootstrap_servers=KAFKA_SERVERS,\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Create S3 connection\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "print(\"Starting simple consumer...\")\n",
    "\n",
    "# Simple loop - just like YouTube tutorial\n",
    "for message in consumer:\n",
    "    earthquake_data = message.value\n",
    "    \n",
    "    # Create filename with earthquake ID\n",
    "    filename = f\"s3://{S3_BUCKET}/earthquake_{earthquake_data['id']}.json\"\n",
    "    \n",
    "    # Write directly to S3\n",
    "    with s3.open(filename, 'w') as file:\n",
    "        json.dump(earthquake_data, file)\n",
    "    \n",
    "    print(f\"Saved earthquake {earthquake_data['id']} to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92522025-2c84-40b0-bb67-ce3692dc2100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in ./my-jupyter-env/lib/python3.13/site-packages (2025.7.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in ./my-jupyter-env/lib/python3.13/site-packages (from s3fs) (2.23.2)\n",
      "Requirement already satisfied: fsspec==2025.7.0 in ./my-jupyter-env/lib/python3.13/site-packages (from s3fs) (2025.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./my-jupyter-env/lib/python3.13/site-packages (from s3fs) (3.12.14)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.39.9,>=1.39.7 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.39.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./my-jupyter-env/lib/python3.13/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./my-jupyter-env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./my-jupyter-env/lib/python3.13/site-packages (from botocore<1.39.9,>=1.39.7->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./my-jupyter-env/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./my-jupyter-env/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2e1fd-91ff-4e90-ba84-9c14cc55bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "with fs.open('earthquake-bucket-muskan/test.json', 'w') as f:\n",
    "    f.write('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd25ca-aa18-4f29-8c0c-845d559df612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from s3fs import S3FileSystem\n",
    "from json import dumps\n",
    "\n",
    "# Simple configuration\n",
    "consumer = KafkaConsumer(\n",
    "    'demo_test234',\n",
    "    bootstrap_servers=['13.204.64.146:9092'],\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "s3 = S3FileSystem(anon=False)\n",
    "\n",
    "print(\"Starting earthquake consumer...\")\n",
    "\n",
    "for message in consumer:\n",
    "    earthquake_data = message.value\n",
    "    \n",
    "    # Create S3 filename\n",
    "    filename = f\"s3://earthquake-bucket-muskan/earthquake_{earthquake_data['id']}.json\"\n",
    "    \n",
    "    # Save to S3\n",
    "    with s3.open(filename, 'w') as file:\n",
    "        json.dump(earthquake_data, file)\n",
    "    \n",
    "    print(f\"Saved earthquake {earthquake_data['id']} to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e55979-f3ba-48dd-9421-30784b3de4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2cfc91a-999c-40d6-b4a0-91c4890fcde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection client_id=kafka-python-2.2.15, node_id=bootstrap-0 host=13.126.75.47:9092 <connecting> [IPv4 ('13.126.75.47', 9092)]>: connecting to 13.126.75.47:9092 [('13.126.75.47', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection client_id=kafka-python-2.2.15, node_id=bootstrap-0 host=13.126.75.47:9092 <checking_api_versions_recv> [IPv4 ('13.126.75.47', 9092)]>: Broker version identified as 2.6\n",
      "INFO:kafka.conn:<BrokerConnection client_id=kafka-python-2.2.15, node_id=bootstrap-0 host=13.126.75.47:9092 <connected> [IPv4 ('13.126.75.47', 9092)]>: Connection complete.\n",
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ('demo_test27',)\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='demo_test27', partition=0)]\n",
      "INFO:kafka.conn:<BrokerConnection client_id=kafka-python-2.2.15, node_id=0 host=13.126.75.47:9092 <connecting> [IPv4 ('13.126.75.47', 9092)]>: connecting to 13.126.75.47:9092 [('13.126.75.47', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection client_id=kafka-python-2.2.15, node_id=0 host=13.126.75.47:9092 <connected> [IPv4 ('13.126.75.47', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection client_id=kafka-python-2.2.15, node_id=bootstrap-0 host=13.126.75.47:9092 <connected> [IPv4 ('13.126.75.47', 9092)]>: Closing connection. \n",
      "INFO:kafka.consumer.fetcher:Resetting offset for partition TopicPartition(topic='demo_test27', partition=0) to offset 0.\n",
      "ERROR:earthquake-consumer:Deserialization failed: Expecting value: line 1 column 1 (char 0) | Data: b'gfjkdvf'\n",
      "INFO:aiobotocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_1.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_2.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_3.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_4.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_5.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_6.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_7.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_8.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_9.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_10.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_11.json\n",
      "INFO:earthquake-consumer:Saved to S3: earthquake-bucket-muskan/quake_12.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m s3 = S3FileSystem(anon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     27\u001b[39m bucket = \u001b[33m'\u001b[39m\u001b[33mearthquake-bucket-muskan\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/consumer/group.py:1188\u001b[39m, in \u001b[36mKafkaConsumer.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1186\u001b[39m     \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28mself\u001b[39m._message_generator_v2()\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1190\u001b[39m     \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/consumer/group.py:1160\u001b[39m, in \u001b[36mKafkaConsumer._message_generator_v2\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1159\u001b[39m     timeout_ms = \u001b[32m1000\u001b[39m * \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m._consumer_timeout - time.time())\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m     record_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six.iteritems(record_map):\n\u001b[32m   1162\u001b[39m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[32m   1163\u001b[39m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[32m   1164\u001b[39m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[32m   1165\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[32m   1166\u001b[39m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[32m   1167\u001b[39m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[32m   1168\u001b[39m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[32m   1169\u001b[39m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/consumer/group.py:684\u001b[39m, in \u001b[36mKafkaConsumer.poll\u001b[39m\u001b[34m(self, timeout_ms, max_records, update_offsets)\u001b[39m\n\u001b[32m    682\u001b[39m timer = Timer(timeout_ms)\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed:\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m     records = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_records\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_offsets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[32m    686\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/consumer/group.py:731\u001b[39m, in \u001b[36mKafkaConsumer._poll_once\u001b[39m\u001b[34m(self, timer, max_records, update_offsets)\u001b[39m\n\u001b[32m    728\u001b[39m     log.debug(\u001b[33m'\u001b[39m\u001b[33mpoll: do not have all fetch positions...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    729\u001b[39m     poll_timeout_ms = \u001b[38;5;28mmin\u001b[39m(poll_timeout_ms, \u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mretry_backoff_ms\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoll_timeout_ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[32m    733\u001b[39m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coordinator.need_rejoin():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/client_async.py:685\u001b[39m, in \u001b[36mKafkaClient.poll\u001b[39m\u001b[34m(self, timeout_ms, future)\u001b[39m\n\u001b[32m    678\u001b[39m         timeout = \u001b[38;5;28mmin\u001b[39m(\n\u001b[32m    679\u001b[39m             user_timeout_ms,\n\u001b[32m    680\u001b[39m             metadata_timeout_ms,\n\u001b[32m    681\u001b[39m             idle_connection_timeout_ms,\n\u001b[32m    682\u001b[39m             request_timeout_ms)\n\u001b[32m    683\u001b[39m         timeout = \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[32m    688\u001b[39m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[32m    689\u001b[39m responses.extend(\u001b[38;5;28mself\u001b[39m._fire_pending_completed_requests())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/my-jupyter-env/lib/python3.13/site-packages/kafka/client_async.py:728\u001b[39m, in \u001b[36mKafkaClient._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    725\u001b[39m \u001b[38;5;28mself\u001b[39m._register_send_sockets()\n\u001b[32m    727\u001b[39m start_select = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ready = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m end_select = time.time()\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sensors:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/selectors.py:452\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    450\u001b[39m ready = []\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from s3fs import S3FileSystem\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"earthquake-consumer\")\n",
    "\n",
    "def safe_deserializer(x):\n",
    "    try:\n",
    "        return json.loads(x.decode('utf-8')) if x else None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Deserialization failed: {e} | Data: {x}\")\n",
    "        return None\n",
    "\n",
    "# Kafka Consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'demo_test27',\n",
    "    bootstrap_servers=['13.126.75.47:9092'],\n",
    "    value_deserializer=safe_deserializer,\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "# S3 Setup\n",
    "s3 = S3FileSystem(anon=False)\n",
    "bucket = 'earthquake-bucket-muskan'\n",
    "\n",
    "for message in consumer:\n",
    "    if message.value:\n",
    "        try:\n",
    "            filename = f\"{bucket}/quake_{message.offset}.json\"\n",
    "            with s3.open(filename, 'w') as f:\n",
    "                f.write(json.dumps(message.value))\n",
    "            logger.info(f\"Saved to S3: {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"S3 upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd0a36-5db5-4f14-aaf7-9aca4050b01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
